{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Document Similarity Analysis\n",
    "## Comparing YouTube Transcript with 2083 Document\n",
    "\n",
    "This notebook presents a comprehensive NLP analysis comparing a YouTube video transcript with a large document (1000+ pages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "with open('2083. EUROPEAN DECLARATION OF INDEPENDENCE.txt', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    doc_text = f.read()\n",
    "\n",
    "with open('youtube_transcript_clean.txt', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    yt_text = f.read()\n",
    "\n",
    "print(f\"Document size: {len(doc_text):,} characters\")\n",
    "print(f\"YouTube transcript size: {len(yt_text):,} characters\")\n",
    "print(f\"\\nDocument words: {len(doc_text.split()):,}\")\n",
    "print(f\"YouTube words: {len(yt_text.split()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Similarity Scores Overview\n",
    "\n",
    "We tested multiple NLP similarity methods to understand the relationship between the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of all similarity scores\n",
    "similarity_results = {\n",
    "    'Method': [\n",
    "        'Bag-of-Words (with stop words)',\n",
    "        'Bag-of-Words (no stop words)',\n",
    "        'Bigrams',\n",
    "        'Trigrams',\n",
    "        'TF-IDF (sklearn)',\n",
    "        'TF-IDF with N-grams'\n",
    "    ],\n",
    "    'Similarity (%)': [91.05, 28.16, 31.14, 0.93, 16.01, 15.31]\n",
    "}\n",
    "\n",
    "df_similarity = pd.DataFrame(similarity_results)\n",
    "print(df_similarity.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.barh(df_similarity['Method'], df_similarity['Similarity (%)'], color='steelblue')\n",
    "ax.set_xlabel('Similarity Score (%)', fontsize=12)\n",
    "ax.set_title('Document Similarity Scores by Method', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    ax.text(width + 1, bar.get_y() + bar.get_height()/2, \n",
    "            f'{width:.2f}%', ha='left', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight:\n",
    "- **Bag-of-words with stop words: 91%** - Artificially inflated by common words\n",
    "- **TF-IDF (most accurate): 16%** - True semantic similarity\n",
    "- **Trigrams: <1%** - Very few exact 3-word phrase matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chunk Analysis\n",
    "\n",
    "Split the 1000+ page document into 4 chunks (~250 pages each) and compared each to the YouTube transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk similarity results\n",
    "chunk_results = {\n",
    "    'Chunk': ['Chunk 1', 'Chunk 2', 'Chunk 3', 'Chunk 4', 'Whole Doc'],\n",
    "    'TF-IDF (%)': [10.94, 13.18, 10.37, 8.52, 16.01],\n",
    "    'N-grams (%)': [7.32, 7.92, 6.87, 5.19, 15.31]\n",
    "}\n",
    "\n",
    "df_chunks = pd.DataFrame(chunk_results)\n",
    "print(df_chunks.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# TF-IDF scores\n",
    "ax1.bar(df_chunks['Chunk'], df_chunks['TF-IDF (%)'], color='coral', alpha=0.7)\n",
    "ax1.set_ylabel('Similarity (%)', fontsize=11)\n",
    "ax1.set_title('TF-IDF Similarity by Chunk', fontsize=12, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# N-gram scores\n",
    "ax2.bar(df_chunks['Chunk'], df_chunks['N-grams (%)'], color='teal', alpha=0.7)\n",
    "ax2.set_ylabel('Similarity (%)', fontsize=11)\n",
    "ax2.set_title('N-gram Similarity by Chunk', fontsize=12, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Finding:\n",
    "**The whole document (16%) is MORE similar than any individual chunk!**\n",
    "\n",
    "This suggests the YouTube video draws themes from across the entire document rather than focusing on one section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vocabulary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text.split()\n",
    "\n",
    "doc_tokens = preprocess_text(doc_text)\n",
    "yt_tokens = preprocess_text(yt_text)\n",
    "\n",
    "doc_vocab = set(doc_tokens)\n",
    "yt_vocab = set(yt_tokens)\n",
    "\n",
    "common_vocab = doc_vocab & yt_vocab\n",
    "yt_only = yt_vocab - doc_vocab\n",
    "doc_only = doc_vocab - yt_vocab\n",
    "\n",
    "vocab_data = {\n",
    "    'Category': ['Common', 'YT Only', 'Doc Only'],\n",
    "    'Count': [len(common_vocab), len(yt_only), len(doc_only)]\n",
    "}\n",
    "\n",
    "df_vocab = pd.DataFrame(vocab_data)\n",
    "\n",
    "# Pie chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['#66b3ff', '#ff9999', '#99ff99']\n",
    "explode = (0.1, 0, 0)\n",
    "\n",
    "ax.pie(df_vocab['Count'], labels=df_vocab['Category'], autopct='%1.1f%%',\n",
    "       colors=colors, explode=explode, startangle=90)\n",
    "ax.set_title('Vocabulary Distribution\\n(YT Transcript: {} unique terms)'.format(len(yt_vocab)), \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVocabulary overlap: {len(common_vocab) / len(yt_vocab) * 100:.1f}%\")\n",
    "print(f\"\\nTerms unique to YT transcript: {len(yt_only)}\")\n",
    "print(\"Sample:\")\n",
    "print(list(yt_only)[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Named Entity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key names and concepts\n",
    "names_data = {\n",
    "    'Name/Concept': ['Marx', 'Shakespeare', 'Frankfurt School', 'Political Correctness', \n",
    "                     'Cultural Marxism', 'Islam', 'Western Civilization'],\n",
    "    'In YT': [1, 1, 0, 0, 0, 0, 1],\n",
    "    'In Document': [14, 11, 63, 58, 28, 186, 0]\n",
    "}\n",
    "\n",
    "df_names = pd.DataFrame(names_data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(df_names['Name/Concept']))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, df_names['In YT'], width, label='YouTube', color='orange', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, df_names['In Document'], width, label='Document', color='blue', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Entity', fontsize=11)\n",
    "ax.set_ylabel('Frequency', fontsize=11)\n",
    "ax.set_title('Named Entity Frequency Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_names['Name/Concept'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observation: Document heavily focuses on Islam (186 mentions),\")\n",
    "print(\"while YT transcript does not mention Islam at all.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Concept Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_data = {\n",
    "    'Concept': ['Political Correctness', 'Cultural Marxism', 'Frankfurt School', \n",
    "                'Post Modernist', 'Western Civilization', 'Freedom of Speech'],\n",
    "    'Document': [78, 28, 72, 0, 0, 2],\n",
    "    'YouTube': [0, 0, 0, 3, 1, 1],\n",
    "    'Status': ['Doc Only', 'Doc Only', 'Doc Only', 'YT Only', 'YT Only', 'Both']\n",
    "}\n",
    "\n",
    "df_concepts = pd.DataFrame(concepts_data)\n",
    "print(df_concepts.to_string(index=False))\n",
    "\n",
    "# Heatmap-style visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create color-coded table\n",
    "for i, concept in enumerate(df_concepts['Concept']):\n",
    "    doc_val = df_concepts.loc[i, 'Document']\n",
    "    yt_val = df_concepts.loc[i, 'YouTube']\n",
    "    \n",
    "    if doc_val > 0:\n",
    "        ax.scatter(0, i, s=doc_val*50, c='blue', alpha=0.6)\n",
    "    if yt_val > 0:\n",
    "        ax.scatter(1, i, s=yt_val*500, c='orange', alpha=0.6)\n",
    "\n",
    "ax.set_yticks(range(len(df_concepts)))\n",
    "ax.set_yticklabels(df_concepts['Concept'])\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels(['Document', 'YouTube'])\n",
    "ax.set_title('Concept Distribution (bubble size = frequency)', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exact Phrase Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_matches = {\n",
    "    'Phrase Length': ['4+ words', '3 words', '2 words'],\n",
    "    'Matches Found': [2, 1, 193],\n",
    "    'Overlap %': [0.28, 0.14, 28.4]\n",
    "}\n",
    "\n",
    "df_phrases = pd.DataFrame(phrase_matches)\n",
    "print(df_phrases.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Notable Exact Matches ===\")\n",
    "print(\"5-word match: 'for the first time in'\")\n",
    "print(\"3-word match: 'throughout the west'\")\n",
    "print(\"3-word match: 'dead white males'\")\n",
    "print(\"\\nThe phrase 'dead white males' appears in both documents\")\n",
    "print(\"in the context of Shakespeare and curriculum changes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions\n",
    "\n",
    "### Summary of Findings:\n",
    "\n",
    "1. **True Similarity: ~16%** (using TF-IDF)\n",
    "   - Initial bag-of-words score of 91% was inflated by common stop words\n",
    "   - More sophisticated methods show moderate similarity\n",
    "\n",
    "2. **Minimal Direct Quotation**\n",
    "   - Only 2 exact matches of 4+ words found\n",
    "   - Less than 1% trigram overlap\n",
    "   - Documents are thematically related but not directly quoting\n",
    "\n",
    "3. **Different Terminology**\n",
    "   - Document uses: \"Political Correctness\", \"Cultural Marxism\"\n",
    "   - YouTube uses: \"Post Modernist\"\n",
    "   - Same concepts, different framing\n",
    "\n",
    "4. **Different Focus**\n",
    "   - Document: Heavy focus on Islam (186 mentions)\n",
    "   - YouTube: Focus on education, post-modernism (no Islam mentions)\n",
    "\n",
    "5. **Distributed Themes**\n",
    "   - Whole document more similar (16%) than any chunk (8-13%)\n",
    "   - YouTube video synthesizes ideas from across entire document\n",
    "\n",
    "### Interpretation:\n",
    "The YouTube transcript and document share **ideological themes and vocabulary** but represent **distinct texts** with different emphases. The video appears to discuss related cultural and educational topics without directly quoting the document."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
