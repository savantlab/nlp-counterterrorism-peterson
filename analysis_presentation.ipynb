{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Document Similarity Analysis\n",
    "## Comparing YouTube Transcript with 2083 Document\n",
    "\n",
    "This notebook presents a comprehensive NLP analysis comparing a YouTube video transcript with a large document (1000+ pages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary of Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "with open('2083. EUROPEAN DECLARATION OF INDEPENDENCE.txt', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    doc_text = f.read()\n",
    "\n",
    "with open('youtube_transcript_clean.txt', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    yt_text = f.read()\n",
    "\n",
    "print(f\"Document size: {len(doc_text):,} characters\")\n",
    "print(f\"YouTube transcript size: {len(yt_text):,} characters\")\n",
    "print(f\"\\nDocument words: {len(doc_text.split()):,}\")\n",
    "print(f\"YouTube words: {len(yt_text.split()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Similarity Scores Overview\n",
    "\n",
    "We tested multiple NLP similarity methods to understand the relationship between the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of all similarity scores\n",
    "similarity_results = {\n",
    "    'Method': [\n",
    "        'Bag-of-Words (with stop words)',\n",
    "        'Bag-of-Words (no stop words)',\n",
    "        'Bigrams',\n",
    "        'Trigrams',\n",
    "        'TF-IDF (sklearn)',\n",
    "        'TF-IDF with N-grams'\n",
    "    ],\n",
    "    'Similarity (%)': [91.05, 28.16, 31.14, 0.93, 16.01, 15.31]\n",
    "}\n",
    "\n",
    "df_similarity = pd.DataFrame(similarity_results)\n",
    "print(df_similarity.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.barh(df_similarity['Method'], df_similarity['Similarity (%)'], color='steelblue')\n",
    "ax.set_xlabel('Similarity Score (%)', fontsize=12)\n",
    "ax.set_title('Document Similarity Scores by Method', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    ax.text(width + 1, bar.get_y() + bar.get_height()/2, \n",
    "            f'{width:.2f}%', ha='left', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight:\n",
    "- **Bag-of-words with stop words: 91%** - Artificially inflated by common words\n",
    "- **TF-IDF (most accurate): 16%** - True semantic similarity\n",
    "- **Trigrams: <1%** - Very few exact 3-word phrase matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chunk Analysis\n",
    "\n",
    "Split the 1000+ page document into 4 chunks (~250 pages each) and compared each to the YouTube transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk similarity results\n",
    "chunk_results = {\n",
    "    'Chunk': ['Chunk 1', 'Chunk 2', 'Chunk 3', 'Chunk 4', 'Whole Doc'],\n",
    "    'TF-IDF (%)': [10.94, 13.18, 10.37, 8.52, 16.01],\n",
    "    'N-grams (%)': [7.32, 7.92, 6.87, 5.19, 15.31]\n",
    "}\n",
    "\n",
    "df_chunks = pd.DataFrame(chunk_results)\n",
    "print(df_chunks.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# TF-IDF scores\n",
    "ax1.bar(df_chunks['Chunk'], df_chunks['TF-IDF (%)'], color='coral', alpha=0.7)\n",
    "ax1.set_ylabel('Similarity (%)', fontsize=11)\n",
    "ax1.set_title('TF-IDF Similarity by Chunk', fontsize=12, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# N-gram scores\n",
    "ax2.bar(df_chunks['Chunk'], df_chunks['N-grams (%)'], color='teal', alpha=0.7)\n",
    "ax2.set_ylabel('Similarity (%)', fontsize=11)\n",
    "ax2.set_title('N-gram Similarity by Chunk', fontsize=12, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Finding:\n",
    "**The whole document (16%) is MORE similar than any individual chunk!**\n",
    "\n",
    "This suggests the YouTube video draws themes from across the entire document rather than focusing on one section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vocabulary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text.split()\n",
    "\n",
    "doc_tokens = preprocess_text(doc_text)\n",
    "yt_tokens = preprocess_text(yt_text)\n",
    "\n",
    "doc_vocab = set(doc_tokens)\n",
    "yt_vocab = set(yt_tokens)\n",
    "\n",
    "common_vocab = doc_vocab & yt_vocab\n",
    "yt_only = yt_vocab - doc_vocab\n",
    "doc_only = doc_vocab - yt_vocab\n",
    "\n",
    "vocab_data = {\n",
    "    'Category': ['Common', 'YT Only', 'Doc Only'],\n",
    "    'Count': [len(common_vocab), len(yt_only), len(doc_only)]\n",
    "}\n",
    "\n",
    "df_vocab = pd.DataFrame(vocab_data)\n",
    "\n",
    "# Pie chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['#66b3ff', '#ff9999', '#99ff99']\n",
    "explode = (0.1, 0, 0)\n",
    "\n",
    "ax.pie(df_vocab['Count'], labels=df_vocab['Category'], autopct='%1.1f%%',\n",
    "       colors=colors, explode=explode, startangle=90)\n",
    "ax.set_title('Vocabulary Distribution\\n(YT Transcript: {} unique terms)'.format(len(yt_vocab)), \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVocabulary overlap: {len(common_vocab) / len(yt_vocab) * 100:.1f}%\")\n",
    "print(f\"\\nTerms unique to YT transcript: {len(yt_only)}\")\n",
    "print(\"Sample:\")\n",
    "print(list(yt_only)[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Word Cloud Visualization\n\n",
    "Visualize the shared vocabulary between documents, sized by frequency in the YouTube transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import re\n",
    "from collections import Counter\n\n",
    "# Stop words list\n",
    "STOP_WORDS = {\n",
    "    'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are',\n",
    "    'arent', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both',\n",
    "    'but', 'by', 'cant', 'cannot', 'could', 'couldnt', 'did', 'didnt', 'do', 'does', 'doesnt',\n",
    "    'doing', 'dont', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadnt',\n",
    "    'has', 'hasnt', 'have', 'havent', 'having', 'he', 'hed', 'hell', 'hes', 'her', 'here',\n",
    "    'heres', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'hows', 'i', 'id', 'ill', 'im',\n",
    "    'ive', 'if', 'in', 'into', 'is', 'isnt', 'it', 'its', 'its', 'itself', 'lets', 'me', 'more',\n",
    "    'most', 'mustnt', 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only',\n",
    "    'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', 'shant',\n",
    "    'she', 'shed', 'shell', 'shes', 'should', 'shouldnt', 'so', 'some', 'such', 'than', 'that',\n",
    "    'thats', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'theres', 'these',\n",
    "    'they', 'theyd', 'theyll', 'theyre', 'theyve', 'this', 'those', 'through', 'to', 'too',\n",
    "    'under', 'until', 'up', 'very', 'was', 'wasnt', 'we', 'wed', 'well', 'were', 'weve', 'werent',\n",
    "    'what', 'whats', 'when', 'whens', 'where', 'wheres', 'which', 'while', 'who', 'whos', 'whom',\n",
    "    'why', 'whys', 'with', 'wont', 'would', 'wouldnt', 'you', 'youd', 'youll', 'youre', 'youve',\n",
    "    'your', 'yours', 'yourself', 'yourselves'\n",
    "}\n\n",
    "# Preprocess and get word frequencies (removing stop words)\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    # Remove stop words and short words\n",
    "    return [t for t in tokens if t not in STOP_WORDS and len(t) > 2]\n\n",
    "doc_tokens = preprocess(doc_text)\n",
    "yt_tokens = preprocess(yt_text)\n\n",
    "doc_vocab = set(doc_tokens)\n",
    "yt_vocab = set(yt_tokens)\n",
    "common_vocab = doc_vocab & yt_vocab\n\n",
    "# Get frequencies from YT transcript (already filtered)\n",
    "yt_freq = Counter(yt_tokens)\n",
    "common_freq = {word: yt_freq[word] for word in common_vocab}\n\n",
    "print(f\"Generating word cloud from {len(common_freq)} shared content words...\\n\")\n",
    "print(f\"(Stop words and words <3 characters removed)\\n\")\n\n",
    "# Create word cloud\n",
    "wordcloud = WordCloud(\n",
    "    width=1400,\n",
    "    height=700,\n",
    "    background_color='white',\n",
    "    colormap='viridis',\n",
    "    relative_scaling=0.5,\n",
    "    min_font_size=10,\n",
    "    max_words=150\n",
    ").generate_from_frequencies(common_freq)\n\n",
    "# Display\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.axis('off')\n",
    "ax.set_title('Shared Vocabulary Word Cloud (242 words, sized by YouTube frequency)',\n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n\n",
    "# Show top words\n",
    "print(\"Top 20 shared words by YouTube frequency:\")\n",
    "for word, freq in sorted(common_freq.items(), key=lambda x: x[1], reverse=True)[:20]:\n",
    "    print(f\"  {word}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Thematic Pattern Analysis\n\n",
    "Clustering shared vocabulary into thematic groups reveals dominant patterns in both documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thematic word groups\n",
    "THEMES = {\n",
    "    'Identity & Social': [\n",
    "        'identity', 'race', 'sex', 'sexual', 'gender', 'diversity', 'equality', 'equity',\n",
    "        'discrimination', 'oppression', 'victim', 'patriarchal', 'class', 'groups', 'group'\n",
    "    ],\n",
    "    'Ideas & Thought': [\n",
    "        'ideas', 'idea', 'think', 'thinking', 'philosophy', 'philosopher', 'ideology',\n",
    "        'concept', 'concepts', 'truth', 'belief', 'believe', 'believers'\n",
    "    ],\n",
    "    'Political & Ideological': [\n",
    "        'marx', 'marxist', 'radical', 'radicals', 'left', 'progressive', 'ideology', 'politics',\n",
    "        'political', 'capitalism', 'socialist', 'communist', 'revolution', 'postmodernism', 'post'\n",
    "    ],\n",
    "    'Education & Academia': [\n",
    "        'education', 'university', 'universities', 'college', 'campus', 'professor', 'professors',\n",
    "        'degree', 'learn', 'student', 'students', 'teaching', 'academic', 'school', 'children'\n",
    "    ],\n",
    "    'Cultural & Western': [\n",
    "        'western', 'west', 'civilization', 'culture', 'cultural', 'society', 'values',\n",
    "        'tradition', 'traditional', 'shakespeare', 'history', 'heritage'\n",
    "    ],\n",
    "    'Power & Conflict': [\n",
    "        'power', 'struggle', 'conflict', 'war', 'fought', 'violence', 'violent', 'violently',\n",
    "        'corrupt', 'dangerous', 'destructive', 'undermine', 'oppose', 'attack'\n",
    "    ],\n",
    "    'Freedom & Rights': [\n",
    "        'freedom', 'free', 'rights', 'speech', 'liberty', 'democracy', 'democratic',\n",
    "        'expression', 'opinion', 'consensus'\n",
    "    ]\n",
    "}\n\n",
    "# Read shared words\n",
    "shared_words = set()\n",
    "with open('shared_words_alphabetical.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line and not line.startswith('SHARED') and not line.startswith('=') and not line.startswith('Total'):\n",
    "            shared_words.add(line)\n\n",
    "# Count frequencies\n",
    "doc_freq = Counter(preprocess_text(doc_text))\n",
    "yt_freq = Counter(preprocess_text(yt_text))\n\n",
    "# Analyze each theme\n",
    "theme_results = []\n",
    "for theme_name, theme_words in THEMES.items():\n",
    "    theme_shared = [w for w in theme_words if w in shared_words]\n",
    "    if not theme_shared:\n",
    "        continue\n",
    "    yt_theme_freq = sum(yt_freq[w] for w in theme_shared)\n",
    "    doc_theme_freq = sum(doc_freq[w] for w in theme_shared)\n",
    "    theme_results.append({\n",
    "        'theme': theme_name,\n",
    "        'words': len(theme_shared),\n",
    "        'yt_freq': yt_theme_freq,\n",
    "        'doc_freq': doc_theme_freq,\n",
    "        'ratio': yt_theme_freq/doc_theme_freq if doc_theme_freq > 0 else 0\n",
    "    })\n\n",
    "df_themes = pd.DataFrame(theme_results)\n",
    "df_themes = df_themes.sort_values('yt_freq', ascending=False)\n\n",
    "# Visualization 1: Theme frequency comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n",
    "# Bar chart of frequencies\n",
    "x = np.arange(len(df_themes))\n",
    "width = 0.35\n",
    "bars1 = ax1.bar(x - width/2, df_themes['yt_freq'], width, label='YouTube', color='coral', alpha=0.8)\n",
    "bars2 = ax1.bar(x + width/2, df_themes['doc_freq']/10, width, label='Document (รท10)', color='steelblue', alpha=0.8)\n",
    "ax1.set_xlabel('Theme', fontsize=11)\n",
    "ax1.set_ylabel('Frequency', fontsize=11)\n",
    "ax1.set_title('Thematic Word Frequency by Document', fontsize=13, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(df_themes['theme'], rotation=45, ha='right', fontsize=9)\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n\n",
    "# YT/Doc ratio chart\n",
    "bars = ax2.barh(df_themes['theme'], df_themes['ratio'], color='teal', alpha=0.7)\n",
    "ax2.set_xlabel('YT/Doc Ratio (relative emphasis)', fontsize=11)\n",
    "ax2.set_title('Thematic Over-representation in YouTube', fontsize=13, fontweight='bold')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    ax2.text(width + 0.002, bar.get_y() + bar.get_height()/2, \n",
    "             f'{width:.4f}', ha='left', va='center', fontsize=9)\n\n",
    "plt.tight_layout()\n",
    "plt.show()\n\n",
    "# Print summary\n",
    "print(\"THEMATIC ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Theme':<30} {'Shared Words':<13} {'YT Freq':<10} {'Doc Freq':<10}\")\n",
    "print(\"-\" * 70)\n",
    "for _, row in df_themes.iterrows():\n",
    "    print(f\"{row['theme']:<30} {row['words']:<13} {row['yt_freq']:<10} {row['doc_freq']:<10}\")\n\n",
    "print(\"\\nTop 3 themes in YouTube transcript:\")\n",
    "for i, (_, row) in enumerate(df_themes.head(3).iterrows(), 1):\n",
    "    print(f\"{i}. {row['theme']} ({row['yt_freq']} occurrences)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight:\n",
    "The thematic clustering reveals that the YouTube transcript emphasizes:\n",
    "1. **Identity & Social** (24 occurrences) - gender, race, oppression, class\n",
    "2. **Ideas & Thought** (17 occurrences) - philosophy, truth, concepts\n",
    "3. **Political & Ideological** (15 occurrences) - Marx, capitalism, radical\n\n",
    "Despite lower absolute frequencies, the YT/Doc ratio shows 'Identity & Social' is ~20% as prominent in the YouTube video as in the document, the highest relative emphasis among all themes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Document Splits Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key names and concepts\n",
    "names_data = {\n",
    "    'Name/Concept': ['Marx', 'Shakespeare', 'Frankfurt School', 'Political Correctness', \n",
    "                     'Cultural Marxism', 'Islam', 'Western Civilization'],\n",
    "    'In YT': [1, 1, 0, 0, 0, 0, 1],\n",
    "    'In Document': [14, 11, 63, 58, 28, 186, 0]\n",
    "}\n",
    "\n",
    "df_names = pd.DataFrame(names_data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(df_names['Name/Concept']))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, df_names['In YT'], width, label='YouTube', color='orange', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, df_names['In Document'], width, label='Document', color='blue', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Entity', fontsize=11)\n",
    "ax.set_ylabel('Frequency', fontsize=11)\n",
    "ax.set_title('Named Entity Frequency Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_names['Name/Concept'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observation: Document heavily focuses on Islam (186 mentions),\")\n",
    "print(\"while YT transcript does not mention Islam at all.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Concept Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_data = {\n",
    "    'Concept': ['Political Correctness', 'Cultural Marxism', 'Frankfurt School', \n",
    "                'Post Modernist', 'Western Civilization', 'Freedom of Speech'],\n",
    "    'Document': [78, 28, 72, 0, 0, 2],\n",
    "    'YouTube': [0, 0, 0, 3, 1, 1],\n",
    "    'Status': ['Doc Only', 'Doc Only', 'Doc Only', 'YT Only', 'YT Only', 'Both']\n",
    "}\n",
    "\n",
    "df_concepts = pd.DataFrame(concepts_data)\n",
    "print(df_concepts.to_string(index=False))\n",
    "\n",
    "# Heatmap-style visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create color-coded table\n",
    "for i, concept in enumerate(df_concepts['Concept']):\n",
    "    doc_val = df_concepts.loc[i, 'Document']\n",
    "    yt_val = df_concepts.loc[i, 'YouTube']\n",
    "    \n",
    "    if doc_val > 0:\n",
    "        ax.scatter(0, i, s=doc_val*50, c='blue', alpha=0.6)\n",
    "    if yt_val > 0:\n",
    "        ax.scatter(1, i, s=yt_val*500, c='orange', alpha=0.6)\n",
    "\n",
    "ax.set_yticks(range(len(df_concepts)))\n",
    "ax.set_yticklabels(df_concepts['Concept'])\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels(['Document', 'YouTube'])\n",
    "ax.set_title('Concept Distribution (bubble size = frequency)', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Split Document Analysis",
    "",
    "Split both documents in half and compare all combinations to identify which sections align most closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split document analysis results\n",
    "split_results = {\n",
    "    'Comparison': [\n",
    "        '2083 Full vs YT Full',\n",
    "        '2083 First Half vs YT Full',\n",
    "        '2083 Full vs YT First Half',\n",
    "        '2083 Second Half vs YT Full',\n",
    "        '2083 Full vs YT Second Half',\n",
    "        '2083 First Half vs YT First Half',\n",
    "        '2083 First Half vs YT Second Half',\n",
    "        '2083 Second Half vs YT Second Half',\n",
    "        '2083 Second Half vs YT First Half'\n",
    "    ],\n",
    "    'Similarity (%)': [16.01, 13.96, 12.67, 11.45, 11.91, 11.75, 9.79, 9.28, 8.35]\n",
    "}\n\n",
    "df_split = pd.DataFrame(split_results)\n",
    "df_split = df_split.sort_values('Similarity (%)', ascending=False)\n",
    "print(df_split.to_string(index=False))\n\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "colors = ['darkblue' if i == 0 else 'steelblue' if i < 3 else 'lightsteelblue' \n",
    "          for i in range(len(df_split))]\n",
    "bars = ax.barh(df_split['Comparison'], df_split['Similarity (%)'], color=colors)\n",
    "ax.set_xlabel('Similarity (%)', fontsize=12)\n",
    "ax.set_title('Split Document Similarity Analysis', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n\n",
    "# Add value labels\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    ax.text(width + 0.3, bar.get_y() + bar.get_height()/2, \n",
    "            f'{width:.2f}%', ha='left', va='center', fontsize=9)\n\n",
    "plt.tight_layout()\n",
    "plt.show()\n\n",
    "print('\\n=== Key Findings ===')\n",
    "print('2083 First Half vs YT Full:   13.96% (MORE similar)')\n",
    "print('2083 Second Half vs YT Full:  11.45% (LESS similar)')\n",
    "print('Difference: 2.51 percentage points')\n",
    "print('\\n\u2192 First ~500 pages of 2083 document more closely match YouTube video content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight:\n",
    "The **first half of the 2083 document** (13.96%) is significantly more similar to the YouTube video than the second half (11.45%). This suggests the video content aligns more closely with themes in the first ~500 pages of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Content Analysis: Feminism in 2083 Document\n\n",
    "Analysis of how feminism is discussed in the 2083 document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n\n",
    "# Feminism-related terms\n",
    "feminism_data = {\n",
    "    'Term': ['feminism', 'feminist', 'patriarchal', 'feminists', 'patriarchy', 'matriarchy', 'sexism', 'misogyny'],\n",
    "    'Count': [23, 13, 13, 10, 6, 6, 3, 1]\n",
    "}\n\n",
    "df_feminism = pd.DataFrame(feminism_data)\n\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n",
    "# Bar chart\n",
    "colors = ['#e74c3c', '#e67e22', '#f39c12', '#f1c40f', '#3498db', '#9b59b6', '#95a5a6', '#7f8c8d']\n",
    "bars = ax1.bar(df_feminism['Term'], df_feminism['Count'], color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "ax1.set_xlabel('Term', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Feminism-Related Terms in 2083 Document', fontsize=14, fontweight='bold', pad=15)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n\n",
    "# Thematic connections\n",
    "themes_data = {\n",
    "    'Connection': ['Political Correctness', 'Cultural Marxism', 'Women\\'s Oppression', 'War (against boys/men)', 'Islam'],\n",
    "    'Mentions': [6, 5, 4, 4, 1]\n",
    "}\n\n",
    "df_themes_fem = pd.DataFrame(themes_data)\n\n",
    "bars2 = ax2.barh(df_themes_fem['Connection'], df_themes_fem['Mentions'], color='coral', alpha=0.8, edgecolor='darkred', linewidth=1.5)\n",
    "ax2.set_xlabel('Number of Connections', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Feminism Thematic Connections', fontsize=14, fontweight='bold', pad=15)\n",
    "ax2.invert_yaxis()\n",
    "ax2.grid(axis='x', alpha=0.3, linestyle='--')\n\n",
    "# Add value labels\n",
    "for bar in bars2:\n",
    "    width = bar.get_width()\n",
    "    ax2.text(width + 0.1, bar.get_y() + bar.get_height()/2,\n",
    "            f'{int(width)}', ha='left', va='center', fontsize=10, fontweight='bold')\n\n",
    "plt.tight_layout()\n",
    "plt.show()\n\n",
    "# Summary statistics\n",
    "total_feminism = df_feminism['Count'].sum()\n",
    "print(f\"Total feminism-related terms: {total_feminism}\")\n",
    "print(f\"\\nTop 3 terms:\")\n",
    "for i, row in df_feminism.head(3).iterrows():\n",
    "    print(f\"  {i+1}. {row['Term']}: {row['Count']}\")\n\n",
    "print(f\"\\nMost common collocation: 'radical feminism' (7 instances)\")\n",
    "print(f\"Primary framing: Cultural Marxism component\")\n",
    "print(f\"\\nKey chapter titles:\")\n",
    "print(f\"  - 'Radical Feminism and Political Correctness'\")\n",
    "print(f\"  - 'The Failure of Western Feminism'\")\n",
    "print(f\"  - 'How the Feminists\\' War against Boys Paved the Way for Islam'\")\n",
    "print(f\"  - 'Feminism Leads to the Oppression of Women'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight:\n",
    "Feminism in the 2083 document is **not discussed as women's rights advocacy**. Instead:\n",
    "- Framed as 'radical feminism' - part of 'Cultural Marxism' ideology\n",
    "- Linked to Political Correctness (6 connections) and Cultural Marxism (5 connections)\n",
    "- Presented as harmful to both women ('Leads to Oppression') and men ('War against Boys')\n",
    "- Connected to Islam's spread in Europe through support for Muslim immigration\n",
    "- Described as 'most destructive and fanatical' element of modern liberalism\n\n",
    "The document portrays feminism as a component of broader ideological attack on Western civilization, not as advocacy for gender equality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Exact Phrase Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_matches = {\n",
    "    'Phrase Length': ['4+ words', '3 words', '2 words'],\n",
    "    'Matches Found': [2, 1, 193],\n",
    "    'Overlap %': [0.28, 0.14, 28.4]\n",
    "}\n",
    "\n",
    "df_phrases = pd.DataFrame(phrase_matches)\n",
    "print(df_phrases.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Notable Exact Matches ===\")\n",
    "print(\"5-word match: 'for the first time in'\")\n",
    "print(\"3-word match: 'throughout the west'\")\n",
    "print(\"3-word match: 'dead white males'\")\n",
    "print(\"\\nThe phrase 'dead white males' appears in both documents\")\n",
    "print(\"in the context of Shakespeare and curriculum changes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusions\n\n",
    "### Summary of Findings:\n\n",
    "1. **True Similarity: ~16%** (using TF-IDF)\n",
    "   - Initial bag-of-words score of 91% was inflated by common stop words\n",
    "   - More sophisticated methods show moderate similarity\n\n",
    "2. **Thematic Patterns** (242 shared content words)\n",
    "   - **Identity & Social**: Most prominent in YouTube (24 occurrences)\n",
    "   - **Ideas & Thought**: Second highest emphasis (17 occurrences)\n",
    "   - **Political & Ideological**: Third major theme (15 occurrences)\n",
    "   - Identity & Social has highest YT/Doc ratio (0.1983x), showing relative over-emphasis\n\n",
    "3. **Minimal Direct Quotation**\n",
    "   - Only 2 exact matches of 4+ words found\n",
    "   - Less than 1% trigram overlap\n",
    "   - Documents are thematically related but not directly quoting\n\n",
    "4. **Different Terminology**\n",
    "   - Document uses: \\\"Political Correctness\\\", \\\"Cultural Marxism\\\"\n",
    "   - YouTube uses: \\\"Post Modernist\\\"\n",
    "   - Same concepts, different framing\n\n",
    "5. **Different Focus**\n",
    "   - Document: Heavy focus on Islam (186 mentions)\n",
    "   - YouTube: Focus on education, identity, post-modernism (no Islam mentions)\n\n",
    "6. **Distributed Themes**\n",
    "   - Whole document more similar (16%) than any chunk (8-13%)\n",
    "   - YouTube video synthesizes ideas from across entire document\n",
    "   - First half of 2083 (13.96%) more aligned than second half (11.45%)\n\n",
    "7. **Content Analysis: Feminism in 2083** (75 occurrences)\n",
    "   - Framed as 'radical feminism' - part of 'Cultural Marxism'\n",
    "   - Linked to Political Correctness (6x) and Cultural Marxism (5x)\n",
    "   - Chapters: 'War against Boys', 'Failure of Western Feminism', 'Oppression of Women'\n",
    "   - Not discussed as women's rights but as ideological threat to Western civilization\n\n",
    "### Interpretation:\n",
    "The YouTube transcript and document share **ideological themes and vocabulary** but represent **distinct texts** with different emphases. The video appears to discuss related cultural and educational topics without directly quoting the document. Thematic analysis reveals the video concentrates on identity politics, philosophical concepts, and political ideology, while the document covers these themes plus extensive material not present in the video (Islam, feminism as Cultural Marxism, etc.)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}